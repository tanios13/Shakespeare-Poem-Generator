{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636b586b",
   "metadata": {},
   "source": [
    "# Synthetise English text character by character using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606417a",
   "metadata": {},
   "source": [
    "In this assignment we train an RNN to synthesize English text character\n",
    "by character. We train a vanilla RNN with outputs, using the text from the book The Goblet of Fire by J.K. Rowling.\n",
    "The variation of SGD you will use for the optimization will be AdaGrad.\n",
    "The final version of the code contains these major components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0a8c8",
   "metadata": {},
   "source": [
    "- Preparing the data: Read in the training data, determine the number\n",
    "of unique characters in the text and set up mapping functions - one\n",
    "mapping each character to a unique index and another mapping each\n",
    "index to a character.\n",
    "- Back-propagation: The forward and the backward pass of the backpropagation\n",
    "algorithm for a vanilla RNN to efficiently compute the\n",
    "gradients.\n",
    "- AdaGrad: updating your RNN's parameters\n",
    "- Synthesizing text from your RNN: Given a learnt set of parameters\n",
    "for the RNN, a default initial hidden state h0 and an initial input\n",
    "vector, x0, from which to bootstrap from then you will write a function\n",
    "to generate a sequence of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2cae9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functions\n",
    "import tensorflow.keras.utils as np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8701c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'goblet_book.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c55c9",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b23a6",
   "metadata": {},
   "source": [
    "Open the text file for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb7cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'r') as fid:\n",
    "    book_data = fid.read()\n",
    "    \n",
    "# Extract all the unique characters from the text\n",
    "book_chars = sorted(set(book_data))\n",
    "# The dimension of the input/ouput data of the RNN\n",
    "K = len(book_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad4b28",
   "metadata": {},
   "source": [
    "Create a dictionnary to convert int to char and char to int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c77a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "for i, char in enumerate(book_chars):\n",
    "    char_to_ind[char] = i\n",
    "    ind_to_char[i] = char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c6dc0b",
   "metadata": {},
   "source": [
    "## Set hyper-parameters & initialize the RNN's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaaf801",
   "metadata": {},
   "source": [
    "Set the hyper parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ff764784",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100 # dimensionality of the hidden layer\n",
    "eta = 0.1 # learning rate\n",
    "seq_length = 25 # length of the input sequence\n",
    "sig = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad7fd7",
   "metadata": {},
   "source": [
    "Initiliaze the RNN's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "60e219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self):\n",
    "        self.b = np.zeros((m,1))\n",
    "        self.c = np.zeros((K,1))\n",
    "        self.U = np.random.randn(m,K)*sig\n",
    "        self.W = np.random.randn(m,m)*sig\n",
    "        self.V = np.random.randn(K,m)*sig\n",
    "        \n",
    "        self.accum_b = np.zeros_like(self.b)\n",
    "        self.accum_c = np.zeros_like(self.c)\n",
    "        self.accum_U = np.zeros_like(self.U)\n",
    "        self.accum_W = np.zeros_like(self.W)\n",
    "        self.accum_V = np.zeros_like(self.V)\n",
    "        \n",
    "    def update_grad(self, t_grad, eps=1e-14):\n",
    "        t_grad.clip_gradient()\n",
    "\n",
    "        self.accum_b += t_grad.b ** 2\n",
    "        self.b -= (eta / np.sqrt(self.accum_b + eps)) * t_grad.b\n",
    "        \n",
    "        self.accum_c += np.square(t_grad.c)\n",
    "        self.c -= (eta / np.sqrt(self.accum_c + eps)) * t_grad.c\n",
    "        \n",
    "        self.accum_U += np.square(t_grad.U)\n",
    "        self.U -= (eta / np.sqrt(self.accum_U + eps)) * t_grad.U\n",
    "        \n",
    "        self.accum_W += np.square(t_grad.W)   \n",
    "        self.W -= (eta / np.sqrt(self.accum_W + eps)) * t_grad.W\n",
    "        \n",
    "        self.accum_V += np.square(t_grad.V)\n",
    "        self.V -= (eta / np.sqrt(self.accum_V + eps)) * t_grad.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "17697294",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59318152",
   "metadata": {},
   "source": [
    "## Synthesize text from your randomly initialized RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532a1ed",
   "metadata": {},
   "source": [
    "Function that will synthesize a sequence of characters using the current parameter values in your RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "27f2d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text(rnn, h_0, x_0, n):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    rnn: parameters of the network\n",
    "    h0: hidden state at time 0\n",
    "    x0: first dummy input to RNN (it can be a character like a fullstop)\n",
    "    n: length of the sequence we want to generate\n",
    "    ______________________________________________\n",
    "    Return:\n",
    "    \n",
    "    return one hot encoding of the list of characters (d x n)\n",
    "    \n",
    "    \"\"\"\n",
    "    h = [h_0]\n",
    "    x = [x_0]\n",
    "    seq = []\n",
    "            \n",
    "    # we genereta the n-1 other characters\n",
    "    for t in range(1, n+1):\n",
    "        a_t = rnn.W @ h[t-1] + (rnn.U @ x[t-1]) + rnn.b\n",
    "        h.append(np.tanh(a_t))\n",
    "        o_t = rnn.V @ h[t] + rnn.c\n",
    "        p_t = functions.softmax(o_t)\n",
    "        \n",
    "        # add character found to sequence\n",
    "        seq.append(np.argmax(p_t))\n",
    "        # randomly select a character based on the output probability scores p \n",
    "        x.append(np_utils.to_categorical(((np.where(np.cumsum(p_t) - np.random.rand() > 0)[0])[0]), K)[:,np.newaxis])\n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ea01bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th'yz2XWthThmPPRWYuL3v3yyMtRhSl¼yvdN.eâdX\"2qQSLR¢Z¼WMq¢3j!QÃhFâgX;Tâ;;mTLPvT)XhMmoR\"¢dvRÃme¢wd(m¼hFeM^¼¢mRWh¼ÃzK¼m?,ew ,CMm;WIi¼vdF¼FzP)gmjzp LQIXyOWdRLW¼X¼Lls3OUM\n",
      "â-mâZ¼\n",
      "oIuv\"UTRCLFPzDXWw a¢oZYh)¢dÃO\n"
     ]
    }
   ],
   "source": [
    "h_0 = np.zeros(m)[:,np.newaxis]\n",
    "x_0 = '.'\n",
    "\n",
    "x_0_onthot = np_utils.to_categorical(char_to_ind[x_0], K)[:,np.newaxis]\n",
    "\n",
    "sentence = [ind_to_char[ind] for ind in synthesize_text(rnn, h_0, x_0_onthot, 200)]\n",
    "print(''.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47422c",
   "metadata": {},
   "source": [
    "## Implement the forward & backward pass of back-prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954aa370",
   "metadata": {},
   "source": [
    "Implement the forward pass for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bc96ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(rnn, x, y, h_0):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    rnn: parameters\n",
    "    x: (K x n), one hot encoded of the input vector\n",
    "    h_0: (1, m), the first hidden state\n",
    "    \n",
    "    \"\"\"\n",
    "    data_size = len(x[0])\n",
    "    h = np.zeros((m, data_size+1))\n",
    "    h[:,0] = h_0\n",
    "    p = np.zeros((K, data_size))\n",
    "    a = np.zeros((m, data_size))\n",
    "\n",
    "    for t in range(1, data_size+1):\n",
    "        a[:, t-1] = (rnn.W @ (h[:, t-1][:,np.newaxis]) + rnn.U @ (x[:,t-1][:,np.newaxis]) + rnn.b)[:,0]\n",
    "        h[:, t] = np.tanh(a[:, t-1])\n",
    "        o_t = rnn.V @ (h[:, t][:,np.newaxis]) + rnn.c\n",
    "        p[:, t-1] = functions.softmax(o_t)[:,0]\n",
    "        \n",
    "    # compute loss\n",
    "    loss = 0\n",
    "    \n",
    "    # for every data in training data set\n",
    "    for d in range(0,len(x[0])):\n",
    "        loss -= y[:,d].T @ np.log(p[:,d])\n",
    "           \n",
    "    return a, h, p, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dd84404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  110.4678595271206\n",
      "predicted:  yv¼jgLmrtt9jLvhO€ty9LDrO-\n",
      "real:  ARRY POTTER AND THE GOBLE\n"
     ]
    }
   ],
   "source": [
    "h_0 = np.zeros(m)\n",
    "x = book_data[0:25]\n",
    "y = book_data[1:26]\n",
    "\n",
    "# convert char to ind\n",
    "x = [char_to_ind[char] for char in x]\n",
    "y = [char_to_ind[char] for char in y]\n",
    "\n",
    "# convert to one hot encoded\n",
    "x = np_utils.to_categorical(x, K).T\n",
    "y = np_utils.to_categorical(y, K).T\n",
    "\n",
    "a, h, p, loss = forward_pass(rnn, x, y, h_0)\n",
    "\n",
    "y_predicted_ind = np.argmax(p, axis=0)\n",
    "y_real_ind = np.argmax(y, axis=0)\n",
    "print(\"loss: \", loss)\n",
    "print(\"predicted: \", ''.join([ind_to_char[char] for char in y_predicted_ind]))\n",
    "print(\"real: \", ''.join([ind_to_char[char] for char in y_real_ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f9b97",
   "metadata": {},
   "source": [
    "Implement the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "aa619825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRADS:\n",
    "    def __init__(self):\n",
    "        self.b = np.zeros((m,1))\n",
    "        self.c = np.zeros((K,1))\n",
    "        self.U = np.zeros((m,K))\n",
    "        self.W = np.zeros((m,m))\n",
    "        self.V = np.zeros((K,m))\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        self.b = np.clip(self.b, -5, 5)\n",
    "        self.c = np.clip(self.c, -5, 5)\n",
    "        self.U = np.clip(self.U, -5, 5)\n",
    "        self.W = np.clip(self.W, -5, 5)\n",
    "        self.V = np.clip(self.V, -5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "621b604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(rnn, y, p, h, a, x):\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    \n",
    "    y: (K x n), one hot encoding of the true label\n",
    "    p: (K x n), one hot encoding of generated label\n",
    "    h: (m x n), hidden state\n",
    "    a: (m x n)\n",
    "    x: (K x n)\n",
    "    \n",
    "    \"\"\"\n",
    "    grads = GRADS()\n",
    "    n = len(y[0])\n",
    "\n",
    "    # propagate through the loss function\n",
    "    g = -(y - p) # (K x n)\n",
    "    for t in range(n):\n",
    "        grads.V += g[:,t][:,np.newaxis] @ h[:,t+1][:,np.newaxis].T\n",
    "    grads.c = np.sum(g, axis=1)[:,np.newaxis]\n",
    "    \n",
    "    # propagate through the h and a\n",
    "    da = np.zeros((m,n))\n",
    "    \n",
    "    dh_t = g[:,n-1][:,np.newaxis].T @ rnn.V # (1 x m)\n",
    "    da_t = np.multiply(dh_t, ((1 - np.tanh(a[:,n-1])**2))) # (1 x m)\n",
    "    da[:,n-1] = da_t\n",
    "    \n",
    "    for t in range(n-2, -1, -1):\n",
    "        dh_t = g[:, t] @ rnn.V + da_t @ rnn.W\n",
    "        da_t = np.multiply(dh_t, ((1 - np.tanh(a[:,t])**2)))\n",
    "        da[:,t] = da_t\n",
    "    \n",
    "    g = da # (m x n)\n",
    "    \n",
    "    for t in range(n):\n",
    "        grads.W += g[:,t][:,np.newaxis] @ h[:,t][:,np.newaxis].T\n",
    "    grads.U = g @ x.T\n",
    "    grads.b = np.sum(g, axis=1)[:,np.newaxis]\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e175d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = compute_gradient(rnn, y, p, h, a, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6d61674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlow(X, Y, h0, W, U, b, V, c, h=1e-4):\n",
    "    grad_W = np.zeros(W.shape)\n",
    "    grad_U = np.zeros(U.shape)\n",
    "    grad_b = np.zeros(b.shape)\n",
    "    grad_V = np.zeros(V.shape)\n",
    "    grad_c = np.zeros(c.shape)\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_try = np.copy(W)\n",
    "            W_try[i, j] -= h \n",
    "            c1 = ComputeCost(X, Y, h0, W_try, U, b, V, c)\n",
    "\n",
    "            W_try = np.copy(W)\n",
    "            W_try[i, j] += h \n",
    "            c2 = ComputeCost(X, Y, h0, W_try, U, b, V, c)\n",
    "\n",
    "            grad_W[i, j] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(U.shape[0]):\n",
    "        for j in range(U.shape[1]):\n",
    "            U_try = np.copy(U)\n",
    "            U_try[i, j] -= h \n",
    "            c1 = ComputeCost(X, Y, h0, W, U_try, b, V, c)\n",
    "\n",
    "            U_try = np.copy(U)\n",
    "            U_try[i, j] += h \n",
    "            c2 = ComputeCost(X, Y, h0, W, U_try, b, V, c)\n",
    "\n",
    "            grad_U[i, j] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(b.shape[0]):\n",
    "        b_try = np.copy(b)\n",
    "        b_try[i, 0] -= h \n",
    "        c1 = ComputeCost(X, Y, h0, W, U, b_try, V, c)\n",
    "\n",
    "        b_try = np.copy(b)\n",
    "        b_try[i, 0] += h \n",
    "        c2 = ComputeCost(X, Y, h0, W, U, b_try, V, c)\n",
    "        \n",
    "        grad_b[i, 0] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            V_try = np.copy(V)\n",
    "            V_try[i, j] -= h \n",
    "            c1 = ComputeCost(X, Y, h0, W, U, b, V_try, c)\n",
    "\n",
    "            V_try = np.copy(V)\n",
    "            V_try[i, j] += h \n",
    "            c2 = ComputeCost(X, Y, h0, W, U, b, V_try, c)\n",
    "\n",
    "            grad_V[i, j] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(c.shape[0]):\n",
    "        c_try = np.copy(c)\n",
    "        c_try[i, 0] -= h \n",
    "        c1 = ComputeCost(X, Y, h0, W, U, b, V, c_try)\n",
    "\n",
    "        c_try = np.copy(c)\n",
    "        c_try[i, 0] += h \n",
    "        c2 = ComputeCost(X, Y, h0, W, U, b, V, c_try)\n",
    "        \n",
    "        grad_c[i, 0] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    return grad_W, grad_U, grad_b, grad_V, grad_c\n",
    "\n",
    "def ComputeCost(x, y, h_0, W, U, b, V, c):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    rnn: parameters\n",
    "    x: (K x n), one hot encoded of the input vector\n",
    "    h_0: (1, m), the first hidden state\n",
    "    \n",
    "    \"\"\"\n",
    "    data_size = len(x[0])\n",
    "    h = np.zeros((m, data_size+1))\n",
    "    h[:,0] = h_0\n",
    "    p = np.zeros((K, data_size))\n",
    "    a = np.zeros((m, data_size))\n",
    "\n",
    "    for t in range(1, data_size+1):\n",
    "        a[:, t-1] = (W @ (h[:, t-1][:,np.newaxis]) + U @ (x[:,t-1][:,np.newaxis]) + b)[:,0]\n",
    "        h[:, t] = np.tanh(a[:, t-1])\n",
    "        o_t = V @ (h[:, t][:,np.newaxis]) + c\n",
    "        p[:, t-1] = functions.softmax(o_t)[:,0]\n",
    "        \n",
    "    # compute loss\n",
    "    loss = 0\n",
    "    \n",
    "    # for every data in training data set\n",
    "    for d in range(0,len(x[0])):\n",
    "        loss -= y[:,d].T @ np.log(p[:,d])\n",
    "           \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ecd50d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W, grad_U, grad_b, grad_V, grad_c = ComputeGradsNumSlow(x, y, h_0, rnn.W, rnn.U, rnn.b, rnn.V, rnn.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "02d179f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W difference:  -4.196708312895897e-10\n",
      "U difference:  -3.4051592795458774e-10\n",
      "V difference:  -3.891596823436816e-10\n",
      "c difference:  1.8859641626178814e-10\n",
      "b difference:  -8.015867414279398e-10\n"
     ]
    }
   ],
   "source": [
    "print(\"W difference: \", np.min(grad_W - grads.W))\n",
    "print(\"U difference: \", np.min(grad_U - grads.U))\n",
    "print(\"V difference: \", np.min(grad_V - grads.V))\n",
    "print(\"c difference: \", np.min(grad_c - grads.c))\n",
    "print(\"b difference: \", np.min(grad_b - grads.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a981ce4",
   "metadata": {},
   "source": [
    "## Train RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "546ee251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(n_epochs):\n",
    "    rnn = RNN() \n",
    "    h_prev = np.zeros(m)\n",
    "    smooth_loss = 0\n",
    "\n",
    "    best_rnn = copy.deepcopy(rnn)\n",
    "    best_h_prev = h_prev.copy()\n",
    "    best_loss = 120\n",
    "    \n",
    "    loss_array = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        h_prev = np.zeros(m)\n",
    "\n",
    "        for e in range(int(len(book_data)/25)):\n",
    "            \n",
    "            # construct the x and y\n",
    "            x = construct_sequence(index=e*seq_length)\n",
    "            y = construct_sequence(index=e*seq_length+1)\n",
    "\n",
    "            # forward pass\n",
    "            a, h, p, loss = forward_pass(rnn, x, y, h_prev)\n",
    "            \n",
    "            # set the h_prev\n",
    "            h_prev = h[:,seq_length]\n",
    "            \n",
    "            # compute loss\n",
    "            if(epoch == 0 and e == 0):\n",
    "                smooth_loss = loss\n",
    "            else:\n",
    "                smooth_loss = 0.999*smooth_loss + loss*0.001\n",
    "\n",
    "            # update gradient using AdaGrad\n",
    "            rnn.update_grad(t_grad=compute_gradient(rnn, y, p, h, a, x))\n",
    "            \n",
    "            if(smooth_loss < best_loss):\n",
    "                best_rnn = copy.deepcopy(rnn)\n",
    "                best_h_prev = h_prev.copy()\n",
    "                best_loss = smooth_loss\n",
    "                                                \n",
    "            if(e % 100 == 0):\n",
    "                loss_array.append(smooth_loss)\n",
    "        \n",
    "            if(e % 10000 == 0):\n",
    "                print(e, \"done from: \", int(len(book_data)/25))\n",
    "                print(\"Loss: \", smooth_loss)\n",
    "                \n",
    "    print(''.join([ind_to_char[ind] for ind in synthesize_text(best_rnn, best_h_prev[:,np.newaxis], x[:,1][:,np.newaxis], n=1000)]))\n",
    "    print(\"Best loss: \", best_loss)\n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "7f72941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sequence(index):\n",
    "    \"\"\"\n",
    "    Returns: one hot label of the char at the index position of the book of length seq_length\n",
    "    \n",
    "    \"\"\"\n",
    "    x = book_data[index:(index+seq_length)]\n",
    "    x = [char_to_ind[char] for char in x]\n",
    "    x = np_utils.to_categorical(x, K).T\n",
    "    return x  \n",
    "\n",
    "def plot_loss(lost):\n",
    "    step_size = 100\n",
    "    x_axis = [i*step_size for i in range(len(lost))] # get the step size values as the x-axis\n",
    "    plt.plot(x_axis, lost, label='Loss')\n",
    "    plt.xlabel('Iter')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "231aa255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done from:  44301\n",
      "Loss:  110.4717571248234\n",
      "10000 done from:  44301\n",
      "Loss:  51.456871762201644\n",
      "20000 done from:  44301\n",
      "Loss:  48.721660557959524\n",
      "30000 done from:  44301\n",
      "Loss:  47.560913415222885\n",
      "40000 done from:  44301\n",
      "Loss:  46.280513785584624\n",
      "0 done from:  44301\n",
      "Loss:  45.36974611708533\n",
      "10000 done from:  44301\n",
      "Loss:  45.826344550508615\n",
      "20000 done from:  44301\n",
      "Loss:  45.1429882565176\n",
      "30000 done from:  44301\n",
      "Loss:  45.27848104547684\n",
      "40000 done from:  44301\n",
      "Loss:  44.46613710188562\n",
      "0 done from:  44301\n",
      "Loss:  43.5191554611222\n",
      "10000 done from:  44301\n",
      "Loss:  44.57550351225718\n",
      "20000 done from:  44301\n",
      "Loss:  44.128658000979065\n",
      "30000 done from:  44301\n",
      "Loss:  44.381263172607795\n",
      "40000 done from:  44301\n",
      "Loss:  43.68688595085664\n",
      "0 done from:  44301\n",
      "Loss:  42.66398120509616\n",
      "10000 done from:  44301\n",
      "Loss:  43.8321810754144\n",
      "20000 done from:  44301\n",
      "Loss:  43.34799837565444\n",
      "30000 done from:  44301\n",
      "Loss:  43.82887312321486\n",
      "40000 done from:  44301\n",
      "Loss:  43.05668863082586\n",
      "0 done from:  44301\n",
      "Loss:  42.015726558256794\n",
      "10000 done from:  44301\n",
      "Loss:  43.302397330510985\n",
      "20000 done from:  44301\n",
      "Loss:  42.84481947749096\n",
      "30000 done from:  44301\n",
      "Loss:  43.40492303932612\n",
      "40000 done from:  44301\n",
      "Loss:  42.616149406770944\n",
      "0 done from:  44301\n",
      "Loss:  41.57250750556137\n",
      "10000 done from:  44301\n",
      "Loss:  42.918516777000704\n",
      "20000 done from:  44301\n",
      "Loss:  42.469551870300535\n",
      "30000 done from:  44301\n",
      "Loss:  43.077862180182635\n",
      "40000 done from:  44301\n",
      "Loss:  42.282619936424105\n",
      "0 done from:  44301\n",
      "Loss:  41.233015820673025\n",
      "10000 done from:  44301\n",
      "Loss:  42.73458780703223\n",
      "20000 done from:  44301\n",
      "Loss:  42.37461431713786\n",
      "30000 done from:  44301\n",
      "Loss:  42.86607921523275\n",
      "40000 done from:  44301\n",
      "Loss:  42.12061007994276\n",
      " . .Ie weokedf s ahey wor a ths sosgrt d and thrner th te tn tnte th tarry   HI haa e toet tumbed s an thesk  ahe song d tf  oec oh k  aotwir  on   H worn terllasked tiroyi tathholeianlre teot ered er \" \"Y  tf the  h shene sot te the  tocent  tot er tf tis h aapirldsrdd s seoad the farthle sd tomte\"\n",
      "\"You hire   of tar  ssai he wast  ng t d thaont d trou tn ti work th thth the fiat aus    . .otg tn tiwenenn  oidhey  aubgt of socn   aou tas tn   Hn or y ane sarrn   said Hrgictont\"  said Hrenhis te ted   . .n ai  teght toe thet taoted te   tnoue wact tis balh the  was s  tumbltengel yte r trary  ah ard taikey  \n",
      "\"Ye sad th the ftarenkito ter neeee  aermhnn  and the s regeid tv rotd tn the son't tp tf tis torkier  oe ernder  Hh dhe wasletettng the  wo ard tarenttd tern tf the sare d ti wad bem bvt rt tf the sas sft an an the e wf the fas thme tfer tiwt ant to t  tou  sare thse s tfer te wad tnteiee lnhd tnard theouk ertrofe   \"  toe tard telandtendleceed toeateng tnsespns nn taech  .\"hll ie\n",
      "Best loss:  40.66163692889274\n"
     ]
    }
   ],
   "source": [
    "loss_array = train_rnn(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "03be65da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca62fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
