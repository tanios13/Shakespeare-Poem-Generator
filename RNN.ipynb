{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636b586b",
   "metadata": {},
   "source": [
    "# Synthetise English text character by character using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606417a",
   "metadata": {},
   "source": [
    "In this assignment we train an RNN to synthesize English text character\n",
    "by character. We train a vanilla RNN with outputs, using the text from the book The Goblet of Fire by J.K. Rowling.\n",
    "The variation of SGD you will use for the optimization will be AdaGrad.\n",
    "The final version of the code contains these major components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0a8c8",
   "metadata": {},
   "source": [
    "- Preparing the data: Read in the training data, determine the number\n",
    "of unique characters in the text and set up mapping functions - one\n",
    "mapping each character to a unique index and another mapping each\n",
    "index to a character.\n",
    "- Back-propagation: The forward and the backward pass of the backpropagation\n",
    "algorithm for a vanilla RNN to efficiently compute the\n",
    "gradients.\n",
    "- AdaGrad: updating your RNN's parameters\n",
    "- Synthesizing text from your RNN: Given a learnt set of parameters\n",
    "for the RNN, a default initial hidden state h0 and an initial input\n",
    "vector, x0, from which to bootstrap from then you will write a function\n",
    "to generate a sequence of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cae9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functions\n",
    "import tensorflow.keras.utils as np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8701c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'goblet_book.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c55c9",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b23a6",
   "metadata": {},
   "source": [
    "Open the text file for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'r') as fid:\n",
    "    book_data = fid.read()\n",
    "    \n",
    "# Extract all the unique characters from the text\n",
    "book_chars = sorted(set(book_data))\n",
    "# The dimension of the input/ouput data of the RNN\n",
    "K = len(book_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad4b28",
   "metadata": {},
   "source": [
    "Create a dictionnary to convert int to char and char to int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "for i, char in enumerate(book_chars):\n",
    "    char_to_ind[char] = i\n",
    "    ind_to_char[i] = char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c6dc0b",
   "metadata": {},
   "source": [
    "## Set hyper-parameters & initialize the RNN's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaaf801",
   "metadata": {},
   "source": [
    "Set the hyper parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff764784",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100 # dimensionality of the hidden layer\n",
    "eta = 0.1 # learning rate\n",
    "seq_length = 25 # length of the input sequence\n",
    "sig = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad7fd7",
   "metadata": {},
   "source": [
    "Initiliaze the RNN's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self):\n",
    "        self.b = np.zeros((m,1))\n",
    "        self.c = np.zeros((K,1))\n",
    "        self.U = np.random.randn(m,K)*sig\n",
    "        self.W = np.random.randn(m,m)*sig\n",
    "        self.V = np.random.randn(K,m)*sig\n",
    "        \n",
    "        self.accum_b = np.zeros_like(self.b)\n",
    "        self.accum_c = np.zeros_like(self.c)\n",
    "        self.accum_U = np.zeros_like(self.U)\n",
    "        self.accum_W = np.zeros_like(self.W)\n",
    "        self.accum_V = np.zeros_like(self.V)\n",
    "        \n",
    "    def update_grad(self, t_grad, eps=1e-14):\n",
    "        t_grad.clip_gradient()\n",
    "\n",
    "        self.accum_b += t_grad.b ** 2\n",
    "        self.b -= (eta / np.sqrt(self.accum_b + eps)) * t_grad.b\n",
    "        \n",
    "        self.accum_c += np.square(t_grad.c)\n",
    "        self.c -= (eta / np.sqrt(self.accum_c + eps)) * t_grad.c\n",
    "        \n",
    "        self.accum_U += np.square(t_grad.U)\n",
    "        self.U -= (eta / np.sqrt(self.accum_U + eps)) * t_grad.U\n",
    "        \n",
    "        self.accum_W += np.square(t_grad.W)   \n",
    "        self.W -= (eta / np.sqrt(self.accum_W + eps)) * t_grad.W\n",
    "        \n",
    "        self.accum_V += np.square(t_grad.V)\n",
    "        self.V -= (eta / np.sqrt(self.accum_V + eps)) * t_grad.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17697294",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59318152",
   "metadata": {},
   "source": [
    "## Synthesize text from your randomly initialized RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532a1ed",
   "metadata": {},
   "source": [
    "Function that will synthesize a sequence of characters using the current parameter values in your RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text(rnn, h_0, x_0, n):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    rnn: parameters of the network\n",
    "    h0: hidden state at time 0\n",
    "    x0: first dummy input to RNN (it can be a character like a fullstop)\n",
    "    n: length of the sequence we want to generate\n",
    "    ______________________________________________\n",
    "    Return:\n",
    "    \n",
    "    return one hot encoding of the list of characters (d x n)\n",
    "    \n",
    "    \"\"\"\n",
    "    h = [h_0]\n",
    "    x = [x_0]\n",
    "    seq = []\n",
    "            \n",
    "    # we genereta the n-1 other characters\n",
    "    for t in range(1, n+1):\n",
    "        a_t = rnn.W @ h[t-1] + (rnn.U @ x[t-1]) + rnn.b\n",
    "        h.append(np.tanh(a_t))\n",
    "        o_t = rnn.V @ h[t] + rnn.c\n",
    "        p_t = functions.softmax(o_t)\n",
    "        \n",
    "        # add character found to next x\n",
    "        cp = np.cumsum(p_t)\n",
    "        a = np.random.rand()\n",
    "        ixs = np.where(cp - a > 0)[0]\n",
    "        x_next = ixs[0]\n",
    "\n",
    "        x.append(np_utils.to_categorical(x_next, num_classes=len(book_chars))[:,np.newaxis])\n",
    "\n",
    "        seq.append(x_next)\n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = np.zeros(m)[:,np.newaxis]\n",
    "x_0 = '.'\n",
    "\n",
    "x_0_onthot = np_utils.to_categorical(char_to_ind[x_0], K)[:,np.newaxis]\n",
    "\n",
    "sentence = [ind_to_char[ind] for ind in synthesize_text(rnn, h_0, x_0_onthot, 200)]\n",
    "print(''.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47422c",
   "metadata": {},
   "source": [
    "## Implement the forward & backward pass of back-prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954aa370",
   "metadata": {},
   "source": [
    "Implement the forward pass for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(rnn, x, y, h_0):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    rnn: parameters\n",
    "    x: (K x n), one hot encoded of the input vector\n",
    "    h_0: (1, m), the first hidden state\n",
    "    \n",
    "    \"\"\"\n",
    "    data_size = len(x[0])\n",
    "    h = np.zeros((m, data_size+1))\n",
    "    h[:,0] = h_0\n",
    "    p = np.zeros((K, data_size))\n",
    "    a = np.zeros((m, data_size))\n",
    "\n",
    "    for t in range(1, data_size+1):\n",
    "        a[:, t-1] = (rnn.W @ (h[:, t-1][:,np.newaxis]) + rnn.U @ (x[:,t-1][:,np.newaxis]) + rnn.b)[:,0]\n",
    "        h[:, t] = np.tanh(a[:, t-1])\n",
    "        o_t = rnn.V @ (h[:, t][:,np.newaxis]) + rnn.c\n",
    "        p[:, t-1] = functions.softmax(o_t)[:,0]\n",
    "        \n",
    "    # compute loss\n",
    "    loss = 0\n",
    "    \n",
    "    # for every data in training data set\n",
    "    for d in range(0,len(x[0])):\n",
    "        loss -= y[:,d].T @ np.log(p[:,d])\n",
    "           \n",
    "    return a, h, p, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_0 = np.zeros(m)\n",
    "x = book_data[0:25]\n",
    "y = book_data[1:26]\n",
    "\n",
    "# convert char to ind\n",
    "x = [char_to_ind[char] for char in x]\n",
    "y = [char_to_ind[char] for char in y]\n",
    "\n",
    "# convert to one hot encoded\n",
    "x = np_utils.to_categorical(x, K).T\n",
    "y = np_utils.to_categorical(y, K).T\n",
    "\n",
    "a, h, p, loss = forward_pass(rnn, x, y, h_0)\n",
    "\n",
    "y_predicted_ind = np.argmax(p, axis=0)\n",
    "y_real_ind = np.argmax(y, axis=0)\n",
    "print(\"loss: \", loss)\n",
    "print(\"predicted: \", ''.join([ind_to_char[char] for char in y_predicted_ind]))\n",
    "print(\"real: \", ''.join([ind_to_char[char] for char in y_real_ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f9b97",
   "metadata": {},
   "source": [
    "Implement the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa619825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRADS:\n",
    "    def __init__(self):\n",
    "        self.b = np.zeros((m,1))\n",
    "        self.c = np.zeros((K,1))\n",
    "        self.U = np.zeros((m,K))\n",
    "        self.W = np.zeros((m,m))\n",
    "        self.V = np.zeros((K,m))\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        self.b = np.clip(self.b, -5, 5)\n",
    "        self.c = np.clip(self.c, -5, 5)\n",
    "        self.U = np.clip(self.U, -5, 5)\n",
    "        self.W = np.clip(self.W, -5, 5)\n",
    "        self.V = np.clip(self.V, -5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(rnn, y, p, h, a, x):\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    \n",
    "    y: (K x n), one hot encoding of the true label\n",
    "    p: (K x n), one hot encoding of generated label\n",
    "    h: (m x n), hidden state\n",
    "    a: (m x n)\n",
    "    x: (K x n)\n",
    "    \n",
    "    \"\"\"\n",
    "    grads = GRADS()\n",
    "    n = len(y[0])\n",
    "\n",
    "    # propagate through the loss function\n",
    "    g = -(y - p) # (K x n)\n",
    "    for t in range(n):\n",
    "        grads.V += g[:,t][:,np.newaxis] @ h[:,t+1][:,np.newaxis].T\n",
    "    grads.c = np.sum(g, axis=1)[:,np.newaxis]\n",
    "    \n",
    "    # propagate through the h and a\n",
    "    da = np.zeros((m,n))\n",
    "    \n",
    "    dh_t = g[:,n-1][:,np.newaxis].T @ rnn.V # (1 x m)\n",
    "    da_t = np.multiply(dh_t, ((1 - np.tanh(a[:,n-1])**2))) # (1 x m)\n",
    "    da[:,n-1] = da_t\n",
    "    \n",
    "    for t in range(n-2, -1, -1):\n",
    "        dh_t = g[:, t] @ rnn.V + da_t @ rnn.W\n",
    "        da_t = np.multiply(dh_t, ((1 - np.tanh(a[:,t])**2)))\n",
    "        da[:,t] = da_t\n",
    "    \n",
    "    g = da # (m x n)\n",
    "    \n",
    "    for t in range(n):\n",
    "        grads.W += g[:,t][:,np.newaxis] @ h[:,t][:,np.newaxis].T\n",
    "    grads.U = g @ x.T\n",
    "    grads.b = np.sum(g, axis=1)[:,np.newaxis]\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = compute_gradient(rnn, y, p, h, a, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d61674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlow(X, Y, h0, W, U, b, V, c, h=1e-4):\n",
    "    grad_W = np.zeros(W.shape)\n",
    "    grad_U = np.zeros(U.shape)\n",
    "    grad_b = np.zeros(b.shape)\n",
    "    grad_V = np.zeros(V.shape)\n",
    "    grad_c = np.zeros(c.shape)\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_try = np.copy(W)\n",
    "            W_try[i, j] -= h \n",
    "            c1 = ComputeCost(X, Y, h0, W_try, U, b, V, c)\n",
    "\n",
    "            W_try = np.copy(W)\n",
    "            W_try[i, j] += h \n",
    "            c2 = ComputeCost(X, Y, h0, W_try, U, b, V, c)\n",
    "\n",
    "            grad_W[i, j] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(U.shape[0]):\n",
    "        for j in range(U.shape[1]):\n",
    "            U_try = np.copy(U)\n",
    "            U_try[i, j] -= h \n",
    "            c1 = ComputeCost(X, Y, h0, W, U_try, b, V, c)\n",
    "\n",
    "            U_try = np.copy(U)\n",
    "            U_try[i, j] += h \n",
    "            c2 = ComputeCost(X, Y, h0, W, U_try, b, V, c)\n",
    "\n",
    "            grad_U[i, j] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(b.shape[0]):\n",
    "        b_try = np.copy(b)\n",
    "        b_try[i, 0] -= h \n",
    "        c1 = ComputeCost(X, Y, h0, W, U, b_try, V, c)\n",
    "\n",
    "        b_try = np.copy(b)\n",
    "        b_try[i, 0] += h \n",
    "        c2 = ComputeCost(X, Y, h0, W, U, b_try, V, c)\n",
    "        \n",
    "        grad_b[i, 0] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            V_try = np.copy(V)\n",
    "            V_try[i, j] -= h \n",
    "            c1 = ComputeCost(X, Y, h0, W, U, b, V_try, c)\n",
    "\n",
    "            V_try = np.copy(V)\n",
    "            V_try[i, j] += h \n",
    "            c2 = ComputeCost(X, Y, h0, W, U, b, V_try, c)\n",
    "\n",
    "            grad_V[i, j] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(c.shape[0]):\n",
    "        c_try = np.copy(c)\n",
    "        c_try[i, 0] -= h \n",
    "        c1 = ComputeCost(X, Y, h0, W, U, b, V, c_try)\n",
    "\n",
    "        c_try = np.copy(c)\n",
    "        c_try[i, 0] += h \n",
    "        c2 = ComputeCost(X, Y, h0, W, U, b, V, c_try)\n",
    "        \n",
    "        grad_c[i, 0] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    return grad_W, grad_U, grad_b, grad_V, grad_c\n",
    "\n",
    "def ComputeCost(x, y, h_0, W, U, b, V, c):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    rnn: parameters\n",
    "    x: (K x n), one hot encoded of the input vector\n",
    "    h_0: (1, m), the first hidden state\n",
    "    \n",
    "    \"\"\"\n",
    "    data_size = len(x[0])\n",
    "    h = np.zeros((m, data_size+1))\n",
    "    h[:,0] = h_0\n",
    "    p = np.zeros((K, data_size))\n",
    "    a = np.zeros((m, data_size))\n",
    "\n",
    "    for t in range(1, data_size+1):\n",
    "        a[:, t-1] = (W @ (h[:, t-1][:,np.newaxis]) + U @ (x[:,t-1][:,np.newaxis]) + b)[:,0]\n",
    "        h[:, t] = np.tanh(a[:, t-1])\n",
    "        o_t = V @ (h[:, t][:,np.newaxis]) + c\n",
    "        p[:, t-1] = functions.softmax(o_t)[:,0]\n",
    "        \n",
    "    # compute loss\n",
    "    loss = 0\n",
    "    \n",
    "    # for every data in training data set\n",
    "    for d in range(0,len(x[0])):\n",
    "        loss -= y[:,d].T @ np.log(p[:,d])\n",
    "           \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd50d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_W, grad_U, grad_b, grad_V, grad_c = ComputeGradsNumSlow(x, y, h_0, rnn.W, rnn.U, rnn.b, rnn.V, rnn.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d179f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W difference: \", np.min(grad_W - grads.W))\n",
    "print(\"U difference: \", np.min(grad_U - grads.U))\n",
    "print(\"V difference: \", np.min(grad_V - grads.V))\n",
    "print(\"c difference: \", np.min(grad_c - grads.c))\n",
    "print(\"b difference: \", np.min(grad_b - grads.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a981ce4",
   "metadata": {},
   "source": [
    "## Train RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ee251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(n_epochs):\n",
    "    rnn = RNN() \n",
    "    h_prev = np.zeros(m)\n",
    "    smooth_loss = 0\n",
    "\n",
    "    best_rnn = copy.deepcopy(rnn)\n",
    "    best_h_prev = h_prev.copy()\n",
    "    best_loss = 120\n",
    "    \n",
    "    loss_array = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        h_prev = np.zeros(m)\n",
    "\n",
    "        for e in range(int(len(book_data)/25)):\n",
    "            \n",
    "            # construct the x and y\n",
    "            x = construct_sequence(index=e*seq_length)\n",
    "            y = construct_sequence(index=e*seq_length+1)\n",
    "\n",
    "            # forward pass\n",
    "            a, h, p, loss = forward_pass(rnn, x, y, h_prev)\n",
    "            \n",
    "            # set the h_prev\n",
    "            h_prev = h[:,seq_length]\n",
    "            \n",
    "            # compute loss\n",
    "            if(epoch == 0 and e == 0):\n",
    "                smooth_loss = loss\n",
    "            else:\n",
    "                smooth_loss = 0.999*smooth_loss + loss*0.001\n",
    "\n",
    "            # update gradient using AdaGrad\n",
    "            rnn.update_grad(t_grad=compute_gradient(rnn, y, p, h, a, x))\n",
    "            \n",
    "            if(smooth_loss < best_loss):\n",
    "                best_rnn = copy.deepcopy(rnn)\n",
    "                best_h_prev = h_prev.copy()\n",
    "                best_loss = smooth_loss\n",
    "                                                \n",
    "            if(e % 100 == 0):\n",
    "                loss_array.append(smooth_loss)\n",
    "        \n",
    "            if(e % 10000 == 0):\n",
    "                print(e, \"done from: \", int(len(book_data)/25))\n",
    "                print(\"Loss: \", smooth_loss)\n",
    "\n",
    "                print(''.join([ind_to_char[ind] for ind in synthesize_text(best_rnn, best_h_prev[:,np.newaxis], x[:,1][:,np.newaxis], n=200)]))\n",
    "\n",
    "                \n",
    "    print(''.join([ind_to_char[ind] for ind in synthesize_text(best_rnn, best_h_prev[:,np.newaxis], x[:,1][:,np.newaxis], n=1000)]))\n",
    "    print(\"Best loss: \", best_loss)\n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sequence(index):\n",
    "    \"\"\"\n",
    "    Returns: one hot label of the char at the index position of the book of length seq_length\n",
    "    \n",
    "    \"\"\"\n",
    "    x = book_data[index:(index+seq_length)]\n",
    "    x = [char_to_ind[char] for char in x]\n",
    "    x = np_utils.to_categorical(x, K).T\n",
    "    return x  \n",
    "\n",
    "def plot_loss(lost):\n",
    "    step_size = 100\n",
    "    x_axis = [i*step_size for i in range(len(lost))] # get the step size values as the x-axis\n",
    "    plt.plot(x_axis, lost, label='Loss')\n",
    "    plt.xlabel('Iter')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231aa255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_array = train_rnn(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be65da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_loss(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca62fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
