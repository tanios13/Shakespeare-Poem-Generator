{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636b586b",
   "metadata": {},
   "source": [
    "# Synthetise English text character by character using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606417a",
   "metadata": {},
   "source": [
    "In this assignment we train an RNN to synthesize English text character\n",
    "by character. We train a vanilla RNN with outputs, using the text from the book The Goblet of Fire by J.K. Rowling.\n",
    "The variation of SGD you will use for the optimization will be AdaGrad.\n",
    "The final version of the code contains these major components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0a8c8",
   "metadata": {},
   "source": [
    "- Preparing the data: Read in the training data, determine the number\n",
    "of unique characters in the text and set up mapping functions - one\n",
    "mapping each character to a unique index and another mapping each\n",
    "index to a character.\n",
    "- Back-propagation: The forward and the backward pass of the backpropagation\n",
    "algorithm for a vanilla RNN to efficiently compute the\n",
    "gradients.\n",
    "- AdaGrad: updating your RNN's parameters\n",
    "- Synthesizing text from your RNN: Given a learnt set of parameters\n",
    "for the RNN, a default initial hidden state h0 and an initial input\n",
    "vector, x0, from which to bootstrap from then you will write a function\n",
    "to generate a sequence of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cae9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import functions\n",
    "import tensorflow.keras.utils as np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8701c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'goblet_book.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c55c9",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b23a6",
   "metadata": {},
   "source": [
    "Open the text file for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb7cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'r') as fid:\n",
    "    book_data = fid.read()\n",
    "    \n",
    "# Extract all the unique characters from the text\n",
    "book_chars = sorted(set(book_data))\n",
    "# The dimension of the input/ouput data of the RNN\n",
    "K = len(book_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad4b28",
   "metadata": {},
   "source": [
    "Create a dictionnary to convert int to char and char to int:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c77a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {}\n",
    "ind_to_char = {}\n",
    "for i, char in enumerate(book_chars):\n",
    "    char_to_ind[char] = i\n",
    "    ind_to_char[i] = char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c6dc0b",
   "metadata": {},
   "source": [
    "## Set hyper-parameters & initialize the RNN's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaaf801",
   "metadata": {},
   "source": [
    "Set the hyper parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff764784",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100 # dimensionality of the hidden layer\n",
    "eta = 0.1 # learning rate\n",
    "seq_length = 25 # length of the input sequence\n",
    "sig = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad7fd7",
   "metadata": {},
   "source": [
    "Initiliaze the RNN's parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self):\n",
    "        self.b = np.zeros((m,1))\n",
    "        self.c = np.zeros((K,1))\n",
    "        self.U = np.random.randn(m,K)*sig\n",
    "        self.W = np.random.randn(m,m)*sig\n",
    "        self.V = np.random.randn(K,m)*sig\n",
    "        \n",
    "        self.accum_b = np.zeros_like(self.b)\n",
    "        self.accum_c = np.zeros_like(self.c)\n",
    "        self.accum_U = np.zeros_like(self.U)\n",
    "        self.accum_W = np.zeros_like(self.W)\n",
    "        self.accum_V = np.zeros_like(self.V)\n",
    "        \n",
    "    def update_grad(self, t_grad, eps=1e-14):\n",
    "        t_grad.clip_gradient()\n",
    "\n",
    "        self.accum_b += t_grad.b ** 2\n",
    "        self.b -= (eta / np.sqrt(self.accum_b + eps)) * t_grad.b\n",
    "        \n",
    "        self.accum_c += np.square(t_grad.c)\n",
    "        self.c -= (eta / np.sqrt(self.accum_c + eps)) * t_grad.c\n",
    "        \n",
    "        self.accum_U += np.square(t_grad.U)\n",
    "        self.U -= (eta / np.sqrt(self.accum_U + eps)) * t_grad.U\n",
    "        \n",
    "        self.accum_W += np.square(t_grad.W)   \n",
    "        self.W -= (eta / np.sqrt(self.accum_W + eps)) * t_grad.W\n",
    "        \n",
    "        self.accum_V += np.square(t_grad.V)\n",
    "        self.V -= (eta / np.sqrt(self.accum_V + eps)) * t_grad.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17697294",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59318152",
   "metadata": {},
   "source": [
    "## Synthesize text from your randomly initialized RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532a1ed",
   "metadata": {},
   "source": [
    "Function that will synthesize a sequence of characters using the current parameter values in your RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27f2d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_text(rnn, h_0, x_0, n):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    rnn: parameters of the network\n",
    "    h0: hidden state at time 0\n",
    "    x0: first dummy input to RNN (it can be a character like a fullstop)\n",
    "    n: length of the sequence we want to generate\n",
    "    ______________________________________________\n",
    "    Return:\n",
    "    \n",
    "    return one hot encoding of the list of characters (d x n)\n",
    "    \n",
    "    \"\"\"\n",
    "    h = [h_0]\n",
    "    x = [x_0]\n",
    "    seq = []\n",
    "            \n",
    "    # we genereta the n-1 other characters\n",
    "    for t in range(1, n+1):\n",
    "        a_t = rnn.W @ h[t-1] + (rnn.U @ x[t-1]) + rnn.b\n",
    "        h.append(np.tanh(a_t))\n",
    "        o_t = rnn.V @ h[t] + rnn.c\n",
    "        p_t = functions.softmax(o_t)\n",
    "        \n",
    "        # add character found to next x\n",
    "        cp = np.cumsum(p_t)\n",
    "        a = np.random.rand()\n",
    "        ixs = np.where(cp - a > 0)[0]\n",
    "        x_next = ixs[0]\n",
    "\n",
    "        x.append(np_utils.to_categorical(x_next, num_classes=len(book_chars))[:,np.newaxis])\n",
    "\n",
    "        seq.append(x_next)\n",
    "        \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea01bec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!7CtW•CWid;zN;\n",
      ",•)Vw3Kd0Mh'hMJ.NX/99rafG\" i37E\"AOKMkAW(73LZdSS\n",
      "K)UT^)kBrGDsQ_qjzFhu?clU_soH\"lf-F'B6iHteUdMcuxI9l'xTEMTWCwo)UIWlmRVbDudBhD: Jw:j24WOfYgMq:XWOQUf1UfHVCZDAEjW^z!iT'o/\n",
      "Uuh'Gns/-ft,\n",
      "t^azC(C\n"
     ]
    }
   ],
   "source": [
    "h_0 = np.zeros(m)[:,np.newaxis]\n",
    "x_0 = '.'\n",
    "\n",
    "x_0_onthot = np_utils.to_categorical(char_to_ind[x_0], K)[:,np.newaxis]\n",
    "\n",
    "sentence = [ind_to_char[ind] for ind in synthesize_text(rnn, h_0, x_0_onthot, 200)]\n",
    "print(''.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47422c",
   "metadata": {},
   "source": [
    "## Implement the forward & backward pass of back-prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954aa370",
   "metadata": {},
   "source": [
    "Implement the forward pass for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc96ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(rnn, x, y, h_0):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    rnn: parameters\n",
    "    x: (K x n), one hot encoded of the input vector\n",
    "    h_0: (1, m), the first hidden state\n",
    "    \n",
    "    \"\"\"\n",
    "    data_size = len(x[0])\n",
    "    h = np.zeros((m, data_size+1))\n",
    "    h[:,0] = h_0\n",
    "    p = np.zeros((K, data_size))\n",
    "    a = np.zeros((m, data_size))\n",
    "\n",
    "    for t in range(1, data_size+1):\n",
    "        a[:, t-1] = (rnn.W @ (h[:, t-1][:,np.newaxis]) + rnn.U @ (x[:,t-1][:,np.newaxis]) + rnn.b)[:,0]\n",
    "        h[:, t] = np.tanh(a[:, t-1])\n",
    "        o_t = rnn.V @ (h[:, t][:,np.newaxis]) + rnn.c\n",
    "        p[:, t-1] = functions.softmax(o_t)[:,0]\n",
    "        \n",
    "    # compute loss\n",
    "    loss = 0\n",
    "    \n",
    "    # for every data in training data set\n",
    "    for d in range(0,len(x[0])):\n",
    "        loss -= y[:,d].T @ np.log(p[:,d])\n",
    "           \n",
    "    return a, h, p, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd84404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  109.55510795334432\n",
      "predicted:  K2DDbKXDGGMDK2VYKGKMK2DBa\n",
      "real:  ARRY POTTER AND THE GOBLE\n"
     ]
    }
   ],
   "source": [
    "h_0 = np.zeros(m)\n",
    "x = book_data[0:25]\n",
    "y = book_data[1:26]\n",
    "\n",
    "# convert char to ind\n",
    "x = [char_to_ind[char] for char in x]\n",
    "y = [char_to_ind[char] for char in y]\n",
    "\n",
    "# convert to one hot encoded\n",
    "x = np_utils.to_categorical(x, K).T\n",
    "y = np_utils.to_categorical(y, K).T\n",
    "\n",
    "a, h, p, loss = forward_pass(rnn, x, y, h_0)\n",
    "\n",
    "y_predicted_ind = np.argmax(p, axis=0)\n",
    "y_real_ind = np.argmax(y, axis=0)\n",
    "print(\"loss: \", loss)\n",
    "print(\"predicted: \", ''.join([ind_to_char[char] for char in y_predicted_ind]))\n",
    "print(\"real: \", ''.join([ind_to_char[char] for char in y_real_ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f9b97",
   "metadata": {},
   "source": [
    "Implement the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa619825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRADS:\n",
    "    def __init__(self):\n",
    "        self.b = np.zeros((m,1))\n",
    "        self.c = np.zeros((K,1))\n",
    "        self.U = np.zeros((m,K))\n",
    "        self.W = np.zeros((m,m))\n",
    "        self.V = np.zeros((K,m))\n",
    "        \n",
    "    def clip_gradient(self):\n",
    "        self.b = np.clip(self.b, -5, 5)\n",
    "        self.c = np.clip(self.c, -5, 5)\n",
    "        self.U = np.clip(self.U, -5, 5)\n",
    "        self.W = np.clip(self.W, -5, 5)\n",
    "        self.V = np.clip(self.V, -5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "621b604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(rnn, y, p, h, a, x):\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    \n",
    "    y: (K x n), one hot encoding of the true label\n",
    "    p: (K x n), one hot encoding of generated label\n",
    "    h: (m x n), hidden state\n",
    "    a: (m x n)\n",
    "    x: (K x n)\n",
    "    \n",
    "    \"\"\"\n",
    "    grads = GRADS()\n",
    "    n = len(y[0])\n",
    "\n",
    "    # propagate through the loss function\n",
    "    g = -(y - p) # (K x n)\n",
    "    for t in range(n):\n",
    "        grads.V += g[:,t][:,np.newaxis] @ h[:,t+1][:,np.newaxis].T\n",
    "    grads.c = np.sum(g, axis=1)[:,np.newaxis]\n",
    "    \n",
    "    # propagate through the h and a\n",
    "    da = np.zeros((m,n))\n",
    "    \n",
    "    dh_t = g[:,n-1][:,np.newaxis].T @ rnn.V # (1 x m)\n",
    "    da_t = np.multiply(dh_t, ((1 - np.tanh(a[:,n-1])**2))) # (1 x m)\n",
    "    da[:,n-1] = da_t\n",
    "    \n",
    "    for t in range(n-2, -1, -1):\n",
    "        dh_t = g[:, t] @ rnn.V + da_t @ rnn.W\n",
    "        da_t = np.multiply(dh_t, ((1 - np.tanh(a[:,t])**2)))\n",
    "        da[:,t] = da_t\n",
    "    \n",
    "    g = da # (m x n)\n",
    "    \n",
    "    for t in range(n):\n",
    "        grads.W += g[:,t][:,np.newaxis] @ h[:,t][:,np.newaxis].T\n",
    "    grads.U = g @ x.T\n",
    "    grads.b = np.sum(g, axis=1)[:,np.newaxis]\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e175d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = compute_gradient(rnn, y, p, h, a, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d61674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeGradsNumSlow(X, Y, h0, W, U, b, V, c, h=1e-4):\n",
    "    grad_W = np.zeros(W.shape)\n",
    "    grad_U = np.zeros(U.shape)\n",
    "    grad_b = np.zeros(b.shape)\n",
    "    grad_V = np.zeros(V.shape)\n",
    "    grad_c = np.zeros(c.shape)\n",
    "\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_try = np.copy(W)\n",
    "            W_try[i, j] -= h \n",
    "            c1 = ComputeCost(X, Y, h0, W_try, U, b, V, c)\n",
    "\n",
    "            W_try = np.copy(W)\n",
    "            W_try[i, j] += h \n",
    "            c2 = ComputeCost(X, Y, h0, W_try, U, b, V, c)\n",
    "\n",
    "            grad_W[i, j] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(U.shape[0]):\n",
    "        for j in range(U.shape[1]):\n",
    "            U_try = np.copy(U)\n",
    "            U_try[i, j] -= h \n",
    "            c1 = ComputeCost(X, Y, h0, W, U_try, b, V, c)\n",
    "\n",
    "            U_try = np.copy(U)\n",
    "            U_try[i, j] += h \n",
    "            c2 = ComputeCost(X, Y, h0, W, U_try, b, V, c)\n",
    "\n",
    "            grad_U[i, j] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(b.shape[0]):\n",
    "        b_try = np.copy(b)\n",
    "        b_try[i, 0] -= h \n",
    "        c1 = ComputeCost(X, Y, h0, W, U, b_try, V, c)\n",
    "\n",
    "        b_try = np.copy(b)\n",
    "        b_try[i, 0] += h \n",
    "        c2 = ComputeCost(X, Y, h0, W, U, b_try, V, c)\n",
    "        \n",
    "        grad_b[i, 0] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            V_try = np.copy(V)\n",
    "            V_try[i, j] -= h \n",
    "            c1 = ComputeCost(X, Y, h0, W, U, b, V_try, c)\n",
    "\n",
    "            V_try = np.copy(V)\n",
    "            V_try[i, j] += h \n",
    "            c2 = ComputeCost(X, Y, h0, W, U, b, V_try, c)\n",
    "\n",
    "            grad_V[i, j] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    for i in range(c.shape[0]):\n",
    "        c_try = np.copy(c)\n",
    "        c_try[i, 0] -= h \n",
    "        c1 = ComputeCost(X, Y, h0, W, U, b, V, c_try)\n",
    "\n",
    "        c_try = np.copy(c)\n",
    "        c_try[i, 0] += h \n",
    "        c2 = ComputeCost(X, Y, h0, W, U, b, V, c_try)\n",
    "        \n",
    "        grad_c[i, 0] = (c2 - c1) / (2*h)\n",
    "    \n",
    "    return grad_W, grad_U, grad_b, grad_V, grad_c\n",
    "\n",
    "def ComputeCost(x, y, h_0, W, U, b, V, c):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    rnn: parameters\n",
    "    x: (K x n), one hot encoded of the input vector\n",
    "    h_0: (1, m), the first hidden state\n",
    "    \n",
    "    \"\"\"\n",
    "    data_size = len(x[0])\n",
    "    h = np.zeros((m, data_size+1))\n",
    "    h[:,0] = h_0\n",
    "    p = np.zeros((K, data_size))\n",
    "    a = np.zeros((m, data_size))\n",
    "\n",
    "    for t in range(1, data_size+1):\n",
    "        a[:, t-1] = (W @ (h[:, t-1][:,np.newaxis]) + U @ (x[:,t-1][:,np.newaxis]) + b)[:,0]\n",
    "        h[:, t] = np.tanh(a[:, t-1])\n",
    "        o_t = V @ (h[:, t][:,np.newaxis]) + c\n",
    "        p[:, t-1] = functions.softmax(o_t)[:,0]\n",
    "        \n",
    "    # compute loss\n",
    "    loss = 0\n",
    "    \n",
    "    # for every data in training data set\n",
    "    for d in range(0,len(x[0])):\n",
    "        loss -= y[:,d].T @ np.log(p[:,d])\n",
    "           \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecd50d6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m grad_W, grad_U, grad_b, grad_V, grad_c \u001b[39m=\u001b[39m ComputeGradsNumSlow(x, y, h_0, rnn\u001b[39m.\u001b[39;49mW, rnn\u001b[39m.\u001b[39;49mU, rnn\u001b[39m.\u001b[39;49mb, rnn\u001b[39m.\u001b[39;49mV, rnn\u001b[39m.\u001b[39;49mc)\n",
      "Cell \u001b[1;32mIn[33], line 16\u001b[0m, in \u001b[0;36mComputeGradsNumSlow\u001b[1;34m(X, Y, h0, W, U, b, V, c, h)\u001b[0m\n\u001b[0;32m     14\u001b[0m         W_try \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(W)\n\u001b[0;32m     15\u001b[0m         W_try[i, j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m h \n\u001b[1;32m---> 16\u001b[0m         c2 \u001b[39m=\u001b[39m ComputeCost(X, Y, h0, W_try, U, b, V, c)\n\u001b[0;32m     18\u001b[0m         grad_W[i, j] \u001b[39m=\u001b[39m (c2 \u001b[39m-\u001b[39m c1) \u001b[39m/\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mh)\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(U\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n",
      "Cell \u001b[1;32mIn[33], line 84\u001b[0m, in \u001b[0;36mComputeCost\u001b[1;34m(x, y, h_0, W, U, b, V, c)\u001b[0m\n\u001b[0;32m     81\u001b[0m a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((m, data_size))\n\u001b[0;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, data_size\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 84\u001b[0m     a[:, t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m (W \u001b[39m@\u001b[39m (h[:, t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:,np\u001b[39m.\u001b[39mnewaxis]) \u001b[39m+\u001b[39m U \u001b[39m@\u001b[39;49m (x[:,t\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][:,np\u001b[39m.\u001b[39;49mnewaxis]) \u001b[39m+\u001b[39m b)[:,\u001b[39m0\u001b[39m]\n\u001b[0;32m     85\u001b[0m     h[:, t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtanh(a[:, t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     86\u001b[0m     o_t \u001b[39m=\u001b[39m V \u001b[39m@\u001b[39m (h[:, t][:,np\u001b[39m.\u001b[39mnewaxis]) \u001b[39m+\u001b[39m c\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grad_W, grad_U, grad_b, grad_V, grad_c = ComputeGradsNumSlow(x, y, h_0, rnn.W, rnn.U, rnn.b, rnn.V, rnn.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02d179f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grad_W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mW difference: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmin(grad_W \u001b[39m-\u001b[39m grads\u001b[39m.\u001b[39mW))\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mU difference: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmin(grad_U \u001b[39m-\u001b[39m grads\u001b[39m.\u001b[39mU))\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mV difference: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mmin(grad_V \u001b[39m-\u001b[39m grads\u001b[39m.\u001b[39mV))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grad_W' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"W difference: \", np.min(grad_W - grads.W))\n",
    "print(\"U difference: \", np.min(grad_U - grads.U))\n",
    "print(\"V difference: \", np.min(grad_V - grads.V))\n",
    "print(\"c difference: \", np.min(grad_c - grads.c))\n",
    "print(\"b difference: \", np.min(grad_b - grads.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a981ce4",
   "metadata": {},
   "source": [
    "## Train RNN using AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "546ee251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(n_epochs):\n",
    "    rnn = RNN() \n",
    "    h_prev = np.zeros(m)\n",
    "    smooth_loss = 0\n",
    "\n",
    "    best_rnn = copy.deepcopy(rnn)\n",
    "    best_h_prev = h_prev.copy()\n",
    "    best_loss = 120\n",
    "    \n",
    "    loss_array = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        h_prev = np.zeros(m)\n",
    "\n",
    "        for e in range(int(len(book_data)/25)):\n",
    "            \n",
    "            # construct the x and y\n",
    "            x = construct_sequence(index=e*seq_length)\n",
    "            y = construct_sequence(index=e*seq_length+1)\n",
    "\n",
    "            # forward pass\n",
    "            a, h, p, loss = forward_pass(rnn, x, y, h_prev)\n",
    "            \n",
    "            # set the h_prev\n",
    "            h_prev = h[:,seq_length]\n",
    "            \n",
    "            # compute loss\n",
    "            if(epoch == 0 and e == 0):\n",
    "                smooth_loss = loss\n",
    "            else:\n",
    "                smooth_loss = 0.999*smooth_loss + loss*0.001\n",
    "\n",
    "            # update gradient using AdaGrad\n",
    "            rnn.update_grad(t_grad=compute_gradient(rnn, y, p, h, a, x))\n",
    "            \n",
    "            if(smooth_loss < best_loss):\n",
    "                best_rnn = copy.deepcopy(rnn)\n",
    "                best_h_prev = h_prev.copy()\n",
    "                best_loss = smooth_loss\n",
    "                                                \n",
    "            if(e % 100 == 0):\n",
    "                loss_array.append(smooth_loss)\n",
    "        \n",
    "            if(e % 10000 == 0):\n",
    "                print(e, \"done from: \", int(len(book_data)/25))\n",
    "                print(\"Loss: \", smooth_loss)\n",
    "\n",
    "                print(''.join([ind_to_char[ind] for ind in synthesize_text(best_rnn, best_h_prev[:,np.newaxis], x[:,1][:,np.newaxis], n=200)]))\n",
    "\n",
    "                \n",
    "    print(''.join([ind_to_char[ind] for ind in synthesize_text(best_rnn, best_h_prev[:,np.newaxis], x[:,1][:,np.newaxis], n=1000)]))\n",
    "    print(\"Best loss: \", best_loss)\n",
    "    return loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f72941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sequence(index):\n",
    "    \"\"\"\n",
    "    Returns: one hot label of the char at the index position of the book of length seq_length\n",
    "    \n",
    "    \"\"\"\n",
    "    x = book_data[index:(index+seq_length)]\n",
    "    x = [char_to_ind[char] for char in x]\n",
    "    x = np_utils.to_categorical(x, K).T\n",
    "    return x  \n",
    "\n",
    "def plot_loss(lost):\n",
    "    step_size = 100\n",
    "    x_axis = [i*step_size for i in range(len(lost))] # get the step size values as the x-axis\n",
    "    plt.plot(x_axis, lost, label='Loss')\n",
    "    plt.xlabel('Iter')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "231aa255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done from:  44301\n",
      "Loss:  109.55122562298119\n",
      "U:zQCA:NFO'yNTfAqn!Gb bT\tTV sEdLsLIPjGFzxSkGqgZNgA•TpDhG_luPCTaYdUg4hAMtJTdPX94jjAMEqGWO\"TVw\n",
      "3rTJOyvutnkh4xG2AWYwcx1ü:'NFPuGJTnP4:YT0ORP}E!T?kwA\n",
      "T4arGa_kT7AtPEA06h\tVA.HdKv:KEfLvP0Gi S/\n",
      "^jeNBeGMGxT\"X4P\n",
      "10000 done from:  44301\n",
      "Loss:  52.27307353783587\n",
      "sass stubp pro ver ealse the buch allarK Easse Ojery, tingt buspees, to storither. \"Whot sumare lele, in thoud of them My ail?\"\n",
      "\"The towers thel Mued hig arouth, Rom. \"Ir mawa nou!\" .'s he ontures row\n",
      "20000 done from:  44301\n",
      "Loss:  51.34678466683489\n",
      "red Poup his a dayf at ichid whe hed bes.\" raice net ope the for his he stintted lithen, and gat he that picteing be doested -He?d  Harry booped hadpur depariofe toding is at tedladt Ho shincat thy ag\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss_array \u001b[39m=\u001b[39m train_rnn(\u001b[39m7\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     19\u001b[0m y \u001b[39m=\u001b[39m construct_sequence(index\u001b[39m=\u001b[39me\u001b[39m*\u001b[39mseq_length\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m a, h, p, loss \u001b[39m=\u001b[39m forward_pass(rnn, x, y, h_prev)\n\u001b[0;32m     24\u001b[0m \u001b[39m# set the h_prev\u001b[39;00m\n\u001b[0;32m     25\u001b[0m h_prev \u001b[39m=\u001b[39m h[:,seq_length]\n",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(rnn, x, y, h_0)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, data_size\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     17\u001b[0m     a[:, t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m (rnn\u001b[39m.\u001b[39mW \u001b[39m@\u001b[39m (h[:, t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:,np\u001b[39m.\u001b[39mnewaxis]) \u001b[39m+\u001b[39m rnn\u001b[39m.\u001b[39mU \u001b[39m@\u001b[39m (x[:,t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:,np\u001b[39m.\u001b[39mnewaxis]) \u001b[39m+\u001b[39m rnn\u001b[39m.\u001b[39mb)[:,\u001b[39m0\u001b[39m]\n\u001b[1;32m---> 18\u001b[0m     h[:, t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtanh(a[:, t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     19\u001b[0m     o_t \u001b[39m=\u001b[39m rnn\u001b[39m.\u001b[39mV \u001b[39m@\u001b[39m (h[:, t][:,np\u001b[39m.\u001b[39mnewaxis]) \u001b[39m+\u001b[39m rnn\u001b[39m.\u001b[39mc\n\u001b[0;32m     20\u001b[0m     p[:, t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m functions\u001b[39m.\u001b[39msoftmax(o_t)[:,\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_array = train_rnn(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be65da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca62fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
